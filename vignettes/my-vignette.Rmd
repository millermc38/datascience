---
title: "The datascience Package: A Tour"
author: "Mark Miller"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
  toc: true
  toc_float: true
  toc_collapsed: true
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",fig.width=5, fig.height=5,cache=T
)
```

### Preface

Please note that this package is still in its development stage; it will likely break on data sets outside of this cozy vignette.

### Introduction

Welcome to the *datascience* package! The goal of this project is to wrap as many different statistical and machine learning methods together as possible without relying on any external resources for calculation. Tidyverse is used simply for data manipulation. In other words, all of the modeling procedures in this package are built on the fundamental operators in statistics: matrix multiplication, division, addition, integration, etc. <span style="color: red;">**I have implemented all of these models from absolute scratch!**</span>

The groundwork for this project was hatched when I first embarked on master's degree in 2018. I contemplated ways to engage with my passion for data science and statistics in a robust manner after graduating (which I did in August of 2020). By building this package, I have the opportunity to explore what are perhaps the three key aegises of statistical and machine learning modeling: theory, coding, and optimization. In July of 2020, I started writing the first functions for this package.

But it is a bit more than this. Often times, these sorts of thoughts have crossed my mind: "What would it be like to do a random forest that split using some other method than binary recursive splitting?" or "what if we used a kernel in KNN?" By building my own package, I have the latitude to tamper with model procedures and sculpt model output the way I want it. Additionally, I've always felt diagnostics and assumption checking tend to recede into the background in the eagerness for analysis. In the long term, I would love to include list objects that house a variety of diagnostic results for any model output.

This project is in its nascency. The model outputs are aesthetically basic (but improving), the code is not vectorized or optimized in several cases, and factors have shaky handling. I've yet to decide if I prefer readability (since this is a package primarily targeted at deliberate practice) or optimization. I suspect the allure of the latter will eventually win out. At any rate, I hope you enjoy this production! It is certainly a delight to make. We begin by loading in the requisite packages and data sets:

```{r setup}
#Install the package from github, and load it into the environment
devtools::install_github("millermc38/datascience",upgrade = "never",force = T,quiet = T)
library(datascience)

#Import a few other packages just so we can compare output
library(caret)
library(asbio)
library(RCurl,quietly = T,verbose = F,warn.conflicts = F)
library(tree,quietly = T) #To compare decision trees with my package
LOESS_data<- read.csv(
  text = getURL("https://raw.githubusercontent.com/millermc38/datascience/master/Data/LOESS.csv"))
data(crabs) #From asbio package
```

Now, let's take a look at some different analysis methods:

### K-means Clustering

The following function conducts k-means clustering using the algorithm that appears in *An Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. It ignores categorical variables. Since there is random assignment at the initiation of the algorithm, it is difficult to compare this to other k-means functions written by other R package authors, but I will continue to search for one. I did notice that if I turn standardization off that sometimes a cluster is dropped. Even when the data is not standardized, it may do this, though I have not observed this case.

Note that I compare the clustering output from my package to the actual species clusters in the classic iris data set. This is only a whimsical comparison, because we should remind ourselves that k-means is unsupervised. The algorithm has no idea what a species is or that it should look for them. Rather, it simply finds similar groups of data. Given the structure of the iris data set, we might guess that it would stumble upon the species, but it is no guarantee. The fact that I know to set *k* to 3 is already a bit of a gratuitous advantage. 

```{r}

#Since the initial cluster assignments are random, we need a seed:
set.seed(2432134)

#Cluster the iris dataset with the datascience package
iris_clustered<-kmeanscluster(data = iris,
                              clusters = 3,
                              max_iterations = 100,
                              print_iterations = T,
                              standardize = T)

# The clusters that the function picked
ggplot()+
  geom_point(data = iris_clustered,aes(x=Sepal.Length,y=Sepal.Width,color=cluster))

#The actual species clusters, just for reference
ggplot()+
  geom_point(data = iris_clustered,aes(x=Sepal.Length,y=Sepal.Width,color=Species))

```


### Linear Regression (OLS)

Linear regression: swift, easy, and effective. But rarely high performance for big data. One cannot build a package called "datascience" without including it.

I compare my package with `lm`. You may see small differences between the numbers, but if you look closely, you can see this is merely due to rounding. In this package, I've made two small tweaks I've long wished `lm` would have: a section describing the categorical levels collapsed into the intercept (in this case, there is only one categorical variable) and user-specified rounding for all estimates (except p-values, which I think are better left in full).

```{r}
#datascience package output
datascience(data = iris,
    family = "gaussian",
    response = c("Petal.Width"),
    covariates = c("Sepal.Length","Species"),
    intercept = T,
    rounding = 5)

#stats package output
lm(formula = Petal.Width~Sepal.Length+Species,
   data = iris)%>%summary
```

### One-way ANOVA

If you've learned ANOVA and regression in a field outside of statistics, you might not be fully aware that ANOVA is actually just a special case of regression wherein all variables are considered as factors. We can use the same call as in the OLS section above. Note that the statistical tests are not all that meaningful here (edits currently underway). In fact, what really distinguished ANOVA from regression is not the model fit process, but generally the analysis that follow. For example, we typically want to look at contrasts. Which groups are different? I endeavor to include these contrast in future updates.

In coding up this function, I ran into one tiny problem. I added an argument to allow users to drop the intercept if those so desired. However, I realized that my $F$ statistic and $R^2$ value no longer matched R's `lm` and `aov`. At first taken aback that I could have a hiccup in such a fundamental statistical tool, I found that I had fallen prey to something of a myth propagated in the popular literature: $R^{2}=\frac{\sum_{i}\left(\hat{y}_{i}-\bar{y}\right)^{2}}{\sum_{i}\left(y_{i}-\bar{y}\right)^{2}}=1-\frac{\sum_{i}\left(y_{i}-\hat{y}_{i}\right)^{2}}{\sum_{i}\left(y_{i}-\bar{y}\right)^{2}}$ is not true! The first middle item is only true when we include the intercept. Furthermore, if we drop the intercept, our regression sum of squares (RSS) gains back the 1 degree of freedom that is usually subtracted out for the intercept. This changes the $F$ statistic, naturally. While I had originally included the option to remove the intercept because this directly gives the user the mean estimates for the levels of the factor we are interested in, it turns out that this changes the analysis itself. I will eventually include an appendix on what *exactly* the degrees of freedom mean in the context of ANOVA and regression (rather than the formula for them, which is nto the same thing!).

```{r}

#datascience package output
datascience(data = iris,
    family = "gaussian",
    response = c("Petal.Width"),
    covariates = c("Species"),
    intercept = T,
    rounding = 5)

#stats package output.
lm(formula = Petal.Width~Species,data = iris,x = T)%>%summary

```

### Power Tests

Power tests are key to conserving resources and preparing for a successful study. This power test is meant for One-way ANOVA. It currently only supports balanced designs. More explanation on this one coming soon!

```{r}
power_test(n_i_per_group = 5,
           n_treatmeants = 4,
           alpha = .05,
           at_least_one_pair_trts_differs_by = .4,
           MSE = .035,
           intercept=T)
```


### Logistic Regression

No commentary here, the datascience package matches R's `glm` perfectly.

```{r}
#datascience package output
datascience(data = mtcars,
            family = "binomial",
            response = c("vs"),
            covariates = c("disp","mpg"),
            rounding = 5)

#stats package output
glm(formula = vs~disp+mpg,
    family = "binomial",
    data = mtcars)%>%summary
```

### Poisson Regression

In the models that follow, my function output matches `glm`'s (you can see the differences are due to rounding). However, my implementation of the Newton-Raphson algorithm to find the maximum likelihood estimates seems to take more steps (9 vs. 6). I'm still exploring why this might be, but clearly it does not impact the results.

```{r}
#datascience package output
datascience(data = crabs,
            family = "poisson",
            response = c("satell"),
            covariates = c("weight","width"),
            rounding = 5)

#stats package output
glm(formula = satell~weight+width,
    family = "poisson",
    data = crabs)%>%summary
```

### LOESS Smoother

Right now, I don't have a comparison plot for this custom LOESS smoother I created, but hopefully I will add that soon! This is a local constant regression. Local linear is in the works. The data fit is simply an x vs. y toy example.

```{r}
datascience(data = LOESS_data,
            response = c("y"),
            covariates = c("x"),
            parameter = 1.5,
            family = "LOESS")
```

### KNN

For KNN, the results below are identical until the 100,000ths place. I am very curious as to what could create such a minute difference on such a small data set (the classic mtcars example). It is possible that caret is doing some rounding that I am not under the hood.

```{r}
#datascience package output
mine<-datascience(data = mtcars,
    family = "knn",
    response = c("drat"),
    covariates = c("disp","mpg"),
    parameter = 3)


#caret package output
theirs<-train(drat~disp+mpg,data = mtcars,method = "knn",
              trControl=trainControl(method = "none"),
              tuneGrid=data.frame(k=3))

#Compare test MSE from these models
comparison<-data.frame(datascience_testMSE=mine$test_MSE,
           caret_testMSE=sqrt(mean((mtcars$drat-predict(object = theirs,newdata = mtcars))^2)))%>%
  set_names(c("datascience","Caret"))%>%round(5)

row.names(comparison)<-"Test MSE"

comparison
```

### CART (bagging, random forest, and boosting are all under development)

This section is currently under development. Right now, my package can create a tree, but it seems to be splitting differently across the feature space than other common packages. 

Trees are known for being easier to understand than regression, which is steeped in theory. This is true, but regression is fantastically efficient. If you dig into the code for my package, you'll see that OLS can be accomplished in just a handful of steps. Regression trees, on the other hand, are difficult to vectorize since they are sequential. Furthermore, their high performance comes from an intensive search for optimal splits across the feature space. Trees test out each branch, covariate, and some subset of the range of each covariate. It's no wonder that individual trees overfit with ease. And such a search naturally translates to quite a bit of looping (which I am trying to cut down on).

I've accomplished the below with just about a minimum viable process. There is clearly some opportunity for improvement: parallelizing branches, writing some Rcpp, removing pipes, not copying data frames, etc. All I can say is hats off to the developers who have made CART implementations so wickedly fast, like from the `ranger` package!

```{r}
#create a decision tree with the datascience package
mine<-cart(data = mtcars,
    family = "tree",
    response = c("hp"),
    covariates = c("disp","mpg"),min_pre_partition_size = 10,min_terminal_node_size = 5,
    print_progress = F)

#Let's use the "tree" package to compare
theirs<-tree(hp~ mpg + disp, 
             data = mtcars,control = tree.control(nrow(mtcars), mincut = 5, minsize = 10))


#It looks like the function I've supplied does not completely imitate the one from 
#the three package just yet
cbind(mine[[1]]$hp,mine[[1]]$mean,predict(theirs))%>%data.frame%>%
  set_names(c("Actual Data","My Tree's Fit","Their Tree's Fit"))

```

