---
title: "The datascience Package: An Introduction"
author: "Mark Miller"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
  toc: true
  toc_float: true
  toc_collapsed: true
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

### Preface

Please note that this package is still in its development stage; it will likely break on datasets outside of this cozy vignette.

### Introduction

Welcome to the *datascience* package! The groundwork for this project was hatched in 2018 as I contemplated ways to engage with my passion for data science and statistics in a robust manner after graduating from my master's degree. By building this package, I have the opportunity to explore what are perhaps the three key ideas behind statistical and machine learning modeling: theory, coding, and optimization. In July of 2020, I started writing the first functions. The goal of this project is to wrap as many different statistical and machine learning methods together as possible without relying on **any** external resources for calculation. In other words, all of the modeling procedures in this package are built on the fundamental operators in statistics: matrix multiplication, division, addition, integration, etc.

But it is a bit more than this. Often times, these sorts of thoughts have crossed my mind: "What would it be like to do a random forest that split using some other method than binary recursive splitting?" or "what if we used a kernel in KNN?" By building my own package, I have the latitude to tamper with model procedures and sculpt model output the way I want it. Additionally, I've always felt diagnostics and assumption checking tend to recede into the background in the eagerness for analysis. In the long term, I would love to include list objects that house a variety of diagnostic results for any model output.

This project is in its nascency. The model outputs are aesthetically basic (but improving), the code is not vectorized or optimized in several cases, and factors have shaky handling. I've yet to decide if I prefer readability (since this is a package primarily targeted at deliberate practice) or optimization. I suspect the allure of the latter will eventually win out. At any rate, I hope you enjoy this production! It is certainly a delight to make.

```{r setup}
#Install the package from github, and load it into the environment
devtools::install_github("millermc38/datascience",upgrade = "never",force = T,quiet = T)
library(datascience)

#Import a few other packages just so we can compare output
library(caret)
library(asbio)
library(RCurl,quietly = T,verbose = F,warn.conflicts = F)
library(tree,quietly = T) #To compare decision trees with my package
LOESS_data<- read.csv(
  text = getURL("https://raw.githubusercontent.com/millermc38/datascience/master/Data/LOESS.csv"))
data(crabs) #From asbio package
```

Now, let's take a look at some different analysis methods:

### K-means Clustering

The following function conducts k-means clustering using the algorithm that appears in *An Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. It ignores categorical variables. Since there is random assignment at the initiation of the algorithm, it is difficult to compare this to other k-means functions written by other R package authors, but I will continue to search for one. I did notice that if I turn standardization off that sometimes a cluster is dropped. Even when the data is not standardized, it may do this, though I have not observed this case.

Note that I compare the clustering process from this package to the actual species clusters. This is only a whimsical comparison, because we should remind ourselves that k-means is unsupervised. The algorithm has no idea what a species is or that it should look for them. Rather, it simply finds similar groups of data. Given the structure of the iris data set, we might guess that it would stumble upon the species, but it is no guarantee.

```{r}

#Since the initial cluster assignments are random, we need a seed:
set.seed(2432134)

#Cluster the iris dataset with the datascience package
iris_clustered<-kmeanscluster(data = iris,
                              clusters = 3,
                              max_iterations = 100,
                              print_iterations = T,
                              standardize = T)

# The clusters that the function picked
ggplot()+
  geom_point(data = iris_clustered,aes(x=Sepal.Length,y=Sepal.Width,color=cluster))

#The actual species clusters, just for reference
ggplot()+
  geom_point(data = iris_clustered,aes(x=Sepal.Length,y=Sepal.Width,color=Species))

```


### Linear Regression (OLS)

Linear regression: swift, easy, and effective. But rarely high performance for big data. One cannot build a package called "datascience" without including it.

I compare my package with `lm`. You may see small differences between the numbers, but if you look closely, you can see this is merely due to rounding. In this package, I've made two small tweaks I've long wished `lm` would have: a section describing the categorical levels collapsed into the intercept (in this case, there is only one categorical variable) and user-specified rounding for all estimates (except p-values, which I think are better left in full).

```{r}
#datascience package output
datascience(data = iris,
    family = "gaussian",
    response = c("Petal.Width"),
    covariates = c("Sepal.Length","Species"),
    intercept = T,
    rounding = 5)

#stats package output
lm(formula = Petal.Width~Sepal.Length+Species,
   data = iris)%>%summary
```

### One-way ANOVA

If you've learned ANOVA and regression in a field outside of statistics, you might not be fully aware that ANOVA is actually just a special case of regression wherein the intercept is dropped and all variables are considered as factors. We can use the same call as in the OLS section, but use the intercept argument (to turn it off). Note that the statistical tests are not all that meaningful here (edits currently underway). In fact, what really distinguished ANOVA from regression is not the model fit process, but generally the analysis that follow. For example, we typically want to look at contrasts. Which groups are different? I endeavor to include these contrast in future updates.

```{r}
#datascience package output
datascience(data = iris,
    family = "gaussian",
    response = c("Petal.Width"),
    covariates = c("Species"),
    intercept = F,
    rounding = 5)

#stats package output
lm(formula = Petal.Width~Species-1,data = iris)%>%summary
```

### Logistic Regression

No commentary here, the datascience package matches R's `glm` perfectly.

```{r}
#datascience package output
datascience(data = mtcars,
            family = "binomial",
            response = c("vs"),
            covariates = c("disp","mpg"),
            rounding = 5)

#stats package output
glm(formula = vs~disp+mpg,
    family = "binomial",
    data = mtcars)%>%summary
```

### Poisson Regression

In the models that follow, my function output matches `glm`'s (you can see the differences are due to rounding). However, my implementation of the Newton-Raphson algorithm to find the maximum likelihood estimates seems to take more steps (9 vs. 6). I'm still exploring why this might be, but clearly it does not impact the results.

```{r}
#datascience package output
datascience(data = crabs,
            family = "poisson",
            response = c("satell"),
            covariates = c("weight","width"),
            rounding = 5)

#stats package output
glm(formula = satell~weight+width,
    family = "poisson",
    data = crabs)%>%summary
```

### LOESS Smoother

Right now, I don't have a comparison plot for this custom LOESS smoother I created, but hopefully I will add that soon! This is a local constant regression. Local linear is in the works.

```{r}
datascience(data = LOESS_data,
            response = c("y"),
            covariates = c("x"),
            parameter = 1.5,
            family = "LOESS")
```

### KNN

For KNN, the results below are identical until the 100,000ths place. I am very curious as to what could create such a minute difference on such a small dataset. It is possible that caret is doing some rounding that I am not under the hood.

```{r}
#datascience package output
mine<-datascience(data = mtcars,
    family = "knn",
    response = c("drat"),
    covariates = c("disp","mpg"),
    parameter = 3)


#caret package output
theirs<-train(drat~disp+mpg,data = mtcars,method = "knn",
              trControl=trainControl(method = "none"),
              tuneGrid=data.frame(k=3))

#Compare test MSE from these models
comparison<-data.frame(datascience_testMSE=mine$test_MSE,
           caret_testMSE=sqrt(mean((mtcars$drat-predict(object = theirs,newdata = mtcars))^2)))%>%
  set_names(c("datascience","Caret"))%>%round(5)

row.names(comparison)<-"Test MSE"

comparison
```

### CART (bagging, random forest, and boosting are all under development)

This section is currently under development. Right now, my package can create a tree, but it seems to be splitting differently across the feature space than other common packages. 

Trees are known for being easier to understand than regression, which is steeped in theory. This is true, but regression is fantastically efficient. If you dig into the code for my package, you'll see that OLS can be accomplished in just a handful of steps. Regression trees, on the other hand, are difficult to vectorize since they are sequential. Furthermore, their high performance comes from an intensive search for optimal splits across the feature space. Trees test out each branch, covariate, and some subset of the range of each covariate. It's no wonder that individual trees overfit with ease. And such a search naturally translates to quite a bit of looping (which I am trying to cut down on).

I've accomplished the below with just about a minimum viable process. There is clearly some opportunity for improvement: parallelizing branches, writing some Rcpp, removing pipes, not copying data frames, etc. All I can say is hats off to the developers who have made CART implementations so wickedly fast, like from the `ranger` package!

```{r}
#create a decision tree with the datascience package
mine<-cart(data = mtcars,
    family = "tree",
    response = c("hp"),
    covariates = c("disp","mpg"),min_pre_partition_size = 10,min_terminal_node_size = 5,
    print_progress = F)

#Let's use the "tree" package to compare
theirs<-tree(hp~ mpg + disp, 
             data = mtcars,control = tree.control(nrow(mtcars), mincut = 5, minsize = 10))


#It looks like the function I've supplied does not completely imitate the one from 
#the three package just yet
cbind(mine[[1]]$hp,mine[[1]]$mean,predict(theirs))%>%data.frame%>%
  set_names(c("Actual Data","My Tree's Fit","Their Tree's Fit"))

```

