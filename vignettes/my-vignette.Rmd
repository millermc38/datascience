---
title: "Data Science Package: An Introduction"
author: "Mark Miller"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
  toc: true
  toc_float: true
  toc_collapsed: true
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

### Preface

Please note that this package is still in its development stage; it will likely break on datasets outside of this cozy vignette.

### Introduction

Welcome to the *datascience* package! The groundwork for this project was hatched in 2018 as I contemplated ways to engage with my passion for data science and statistics in a robust manner after graduating from my master's degree. By building this package, I have the opportunity to explore what are perhaps the three key ideas behind statistical and machine learning modeling: theory, coding, and optimization. In July of 2020, I started writing the first functions. The goal of this project is to wrap as many different statistical and machine learning methods together as possible without relying on **any** external resources for calculation. In other words, all of the modeling procedures in this package are built on the fundamental operators in statistics: matrix multiplication, division, addition, integration, etc.

But it is a bit more than this. Often times, these sorts of thoughts have crossed my mind: "What would it be like to do a random forest that split using some other method than binary recursive splitting?" or "what if we used a kernel in KNN?" By building my own package, I have the latitude to tamper with model procedures and sculpt model output the way I want it. Additionally, I've always felt diagnostics and assumption checking tend recede into the background in the eagerness for analysis. In the long term, I would love to include list objects that house a variety of diagnostic results for any model output.

This project is in its nascency. The model outputs are not well formatted, the code is not vectorized or optimized, factors have shaky handling, and the package requires Tidyverse. This last item is a bit of a no-no is the package building world, and pipes tend to be slow in evaluating. I've yet to decide if I prefer readability (since this is a package primarily targeted at deliberate practice) or optimization. I suspect the allure of the latter will eventually win out. At any rate, I hope you enjoy this production! It is certainly a delight to make.

```{r setup}
#Install the package from github, and load it into the environment
devtools::install_github("millermc38/datascience",upgrade = "never",force = T,quiet = T)
library(datascience)

#Import a few other packages just so we can compare output
library(caret)
library(asbio)
library(RCurl,quietly = T)
library(tree,quietly = T) #To compare decision trees with my package
LOESS_data<- read.csv(
  text = getURL("https://raw.githubusercontent.com/millermc38/datascience/master/Data/LOESS.csv"))
data(crabs) #From asbio package
```

Now, let's take a look at some different analysis methods:

### K-Means Clustering

The following function conducts k-means clustering using the algorithm that appears in *An Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. It ignores categorical variables. Since there is random assignment, it is difficult to compare this to other k-means functions, but I will continue to search for one. I did notice that if I turn standardization off that sometimes a cluster is dropped. Even when when the data is not standardized, sometimes the third cluster would just be a single point.

```{r}

#Since the initial cluster assignments are random, we need a seed:
set.seed(2432134)

#Cluster the iris dataset with the datascience package
iris_clustered<-kmeanscluster(data = iris,
                              clusters = 3,
                              max_iterations = 100,
                              print_iterations = T,
                              standardize = T)

# The clusters that the function picked
ggplot()+
  geom_point(data = iris_clustered,aes(x=Sepal.Length,y=Sepal.Width,color=cluster))

#The actual species clusters, just for reference
ggplot()+
  geom_point(data = iris_clustered,aes(x=Sepal.Length,y=Sepal.Width,color=Species))

```


### Linear Regression (OLS)

Ah, linear regression. Fast, easy, and effective. But rarely high performance for big data. How could I build a package that doesn't have it?

I compare my package with `lm`. Here, there are small differences in the standard errors between these two packages, but not the $\beta$ estimates. Granted they are minute differences, but it is is still interesting to note. I will explore this further.

```{r}
#datascience package output
datascience(data = iris,
    family = "gaussian",
    response = c("Petal.Width"),
    covariates = c("Sepal.Length","Species"),
    intercept = T,
    rounding = 5)

#stats package output
lm(formula = Petal.Width~Sepal.Length+Species,
   data = iris)%>%summary
```

### ANOVA

If you've learned ANOVA and regression in a field outside of statistics, you might not be fully aware that ANOVA is actually just a special case of regression wherein the intercept is dropped and all variables are considered as factors. We can use the same call as in the OLS section, but use the intercept argument (to turn it off). Note that the statistical tests are not all that meaningful here (edits currently underway). In fact, what really distinguished ANOVA from regression is not the model, but generally the analysis that follow. For example, we typically want to look at contrasts. Which groups are different?

```{r}
#datascience package output
datascience(data = iris,
    family = "gaussian",
    response = c("Petal.Width"),
    covariates = c("Species"),
    intercept = F,
    rounding = 5)

#stats package output
lm(formula = Petal.Width~Species-1,data = iris)%>%summary
```

### Power Tests for One-Way ANOVA

In ANOVA, we essentially look at how much variance from treatments we have vs. how much variance we have from all of the factor level means. In other words, we have $\frac{MSE_{treatments}}{MSE}$. Naturally, $MSE$~$\chi^2$ distributed (or as my students liked to call is, "Chee-squared"). But when we take the ratio of them, we get an $F$ distribution (proof omitted here). However, this F-distribution has a different appearance depending on whether the null or alternative is true. If the null is true--that is, none of the group means differ--then the ratio follows a central F_(a-1,n-a) distribution. The degrees of freedom are easy to rememeber: the first, $a-1$ refers to the fact that for each treatment estimation (there are $a$ treatments), we lose a degree of freedom. Likewise, in the denominator represents the within group MSE, which we know from the basic assumptions of regression (of which ANOVA is a special case) that errors are normally distributed with a common variance (in regression, we say that the distribution is normal regardless of X). We have $a$ means estimated, so $a$ degrees of freedom lost in that estimate.




```{r}
set.seed(1)
n<-50
fake_ANOVA_data<-data.frame(group1=rnorm(n = n, mean = 0, sd = 1),
                            group2=rnorm(n = n, mean = 1, sd = 1),
                            group3=rnorm(n = n, mean = 2, sd = 1))%>%
  pivot_longer(names_to = "group",values_to = "value",cols = c("group1","group2","group3"))%>%
  data.frame%>%group_by(group)%>%summarize(mean=mean(value))

datascience(data = fake_ANOVA_data,response = c("value"),
            covariates = c("group"),
            family = "gaussian",
            intercept = F)

datascience(data = iris,
    family = "gaussian",
    response = c("Petal.Width"),
    covariates = c("Species"),
    intercept = F)

null_F<-rf


#Get mses, means are same
```



### Logistic Regression

This one is a perfect match in terms of results!

```{r}
#datascience package output
datascience(data = mtcars,
            family = "binomial",
            response = c("vs"),
            covariates = c("disp","mpg"),
            rounding = 5)

#stats package output
glm(formula = vs~disp+mpg,
    family = "binomial",
    data = mtcars)%>%summary
```


### Poisson Regression

In the models that follow, my function output matches `glm`'s (you can see the differences are due to rounding). However, my implementation of the Newton-Raphson algorithm to find the maximum likelihood estimates seems to take more steps (9 vs. 6). I'm still exploring why this might be, but clearly it does not impact the results.

```{r}
#datascience package output
datascience(data = crabs,
            family = "poisson",
            response = c("satell"),
            covariates = c("weight","width"),
            rounding = 5)

#stats package output
glm(formula = satell~weight+width,
    family = "poisson",
    data = crabs)%>%summary
```

### LOESS Smoother

Right now, I don't have a comparison plot for this custom LOESS smoother I created, but hopefully I will add that soon! This is a local constant regression. Local linear is in the works.

```{r}
datascience(data = LOESS_data,
            response = c("y"),
            covariates = c("x"),
            parameter = 1.5,
            family = "LOESS")
```

### KNN

For KNN, the results below are identical until the 100,000ths place. Good enough for me! Note that the parameter argument here is the number of neighbors.

```{r}
#datascience package output
mine<-datascience(data = mtcars,
    family = "knn",
    response = c("drat"),
    covariates = c("disp","mpg"),
    parameter = 3)


#caret package output
theirs<-train(drat~disp+mpg,data = mtcars,method = "knn",
              trControl=trainControl(method = "none"),
              tuneGrid=data.frame(k=3))

#Compare test MSE from these models
data.frame(datascience_testMSE=mine$test_MSE,
           caret_testMSE=sqrt(mean((mtcars$drat-predict(object = theirs,newdata = mtcars))^2)))
```

### CART (bagging, random forest, boosting all under development)

This is currently under development. Right now, the package can create a tree, but it seems to be splitting differently across the feature space than other common packages. 

Trees are known for being easier to understand than regression, which is steeped in theory. This is true, but regression is fantastically efficient. If you dig into the code for my package, you'll see that OLS can be accomplished in just a handful of steps. Regression trees, on the other hand, are difficult to vectorize since they are sequential. Furthermore, their high performance comes from an intensive search for optimal splits across the feature space. Trees test out each branch, covariate, and some subset of the range of each covariate. It's no wonder that individual trees overfit with ease. ANd such a search naturally translates to quite a bit of looping (which I am trying to cut down on).

You can see that my code for this is just about a minimum viable process. There is clearly some opportunity for improvement: parallelizing branches, writing some Rcpp, removing pipes, not copying dataframes, etc. All I can say is hats off to the developers who have made CART implementations so wickedly fast, like from the `ranger` package!

```{r}
#We'll start by making a decision tree
mine<-cart(data = mtcars,
    family = "tree",
    response = c("hp"),
    covariates = c("disp","mpg"),min_pre_partition_size = 10,min_terminal_node_size = 5,
    print_progress = F)

#Let's use the "tree" package to compare
theirs<-tree(hp~ mpg + disp, 
             data = mtcars,control = tree.control(nrow(mtcars), mincut = 5, minsize = 10))


#It looks like the function I've supplied does not completely imitate the one from 
#the three package just yet
cbind(mine[[1]]$hp,mine[[1]]$mean,predict(theirs))%>%data.frame%>%
  set_names(c("Actual Data","My Tree's Fit","Their Tree's Fit"))

```

