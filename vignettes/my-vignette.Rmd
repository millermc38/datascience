---
title: "Data Science Package: An Introduction"
author: "Mark Miller"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

### Preface

Please note that this package is still in its development stage; it will likely break on datasets outside of this cozy vignette.

### Introduction

Welcome to the Data Science package! The goal of this project is to wrap as many different statistical and machine learning methods together as possible without relying on ANY external resources for calculation. In other words, all of the modeling procedures in this package are built on the fundamental operators in statistics: matrix multiplication, division, addition, integration, etc. This package only requires other packages (outside of Tidyverse, more on that soon) for the purpose of model comparison. In this sense, the motivation for this work is purely borne out of a desire to merge theory and coding, thereby keeping my statistical chops in good shape.

But it is a bit more than this. Often times, these sorts of thoughts have crossed my mind: "What would it be like to do a random forest that split using some other method than binary recursive splitting?" or "what if we used a kernel in KNN?" By building my own package, I have the latitude to tamper with model procedures. And make model output the way I want it.

Additionally, I've always felt diagnostics and assumption checking tend recede into the background in the eagerness for analysis. In the long term, I would love to include list objects that house a variety of diagnostic results for any model output.

This project is in its nascency. The model outputs are not well formatted, the code is not vectorized, factors have shaky handling, and the package requires Tidyverse. This last item is a bit of a no-no is the package building world, and pipes tend to be slow in evaluating. I've yet to decide if I prefer readability (since this is a package primarily targeted at deliberate practice) or optimization. I suspect the allure of the latter will eventually win out. At any rate, I hope you enjoy this production! It is certainly a delight to make.

```{r setup}
#Install the package from github, and load it into the environment
devtools::install_github("millermc38/datascience",upgrade = "never",force = T,quiet = T)
library(datascience)

#Import a few other packages just so we can compare output
library(caret)
library(asbio)
library(RCurl)
library(tree) #To compare decision trees with my package
LOESS_data<- read.csv(text = getURL("https://raw.githubusercontent.com/millermc38/datascience/master/Data/LOESS.csv"))
data(crabs) #From asbio package
```

Now, let's take a look at some different analysis methods:

### Linear Regression (OLS)

For OLS, there are small differences in the standard errors between these two packages, but not the $\beta$ estimates. Granted they are minute differences, but it is is still interesting to note.

```{r}
#datascience package output
datascience(data = iris,
    family = "gaussian",
    response = c("Petal.Width"),
    covariates = c("Sepal.Length","Species"))

#stats package output
lm(formula = Petal.Width~Sepal.Length+Species,
   data = iris)%>%summary
```

### ANOVA (under development)

If you've learned ANOVA and regression in a field outside of statistics, you might not be fully aware that ANOVA is actually just a special case of regression wherein the intercept is dropped and all variables are considered as factors. We can actually use the same call as in the OLS section, but use the intercept argument. Note that the statistical tests are not all that meaningful here (edits currently underway). In fact, what really distinguished ANOVA from regression is not the model, but generally the analysis that follow. For example, we typically want to look at contrasts. Which groups are different?

```{r}
datascience(data = iris,
    family = "gaussian",
    response = c("Petal.Width"),
    covariates = c("Species"),
    intercept = F)

lm(formula = Petal.Width~Species-1,data = iris)%>%summary
```


### Logistic Regression

This one is a perfect match in terms of results!

```{r}
#datascience package output
datascience(data = mtcars,
            family = "binomial",
            response = c("vs"),
            covariates = c("disp","mpg"))

#stats package output
glm(formula = vs~disp+mpg,
    family = "binomial",
    data = mtcars)%>%summary
```


### Poisson Regression

In the models that follow, my function output matches glm's (you can see the differences are due to rounding). However, my implementation of the Newton-Raphson algorithm to find the maximum likelihood estimates seems to take more steps (9 vs. 6). I'm still exploring why this might be, but clearly it does not impact the results.

```{r}
#datascience package output
datascience(data = crabs,
            family = "poisson",
            response = c("satell"),
            covariates = c("weight","width"))

#stats package output
glm(formula = satell~weight+width,
    family = "poisson",
    data = crabs)%>%summary

```

### LOESS Smoother

Right now, I don't have a comparison plot for this custom LOESS smoother, but hopefully I will add that soon! This is a local constant regression. Local linear is in the works.

```{r}
datascience(data = LOESS_data,
            response = c("y"),
            covariates = c("x"),
            parameter = 1.5,
            family = "LOESS")
```

### KNN

For KNN, the results are identical until the 100,000ths place. Good enough for me! Note that the parameter argument here is the number of neighbors.

```{r}
#datascience package output
mine<-datascience(data = mtcars,
    family = "knn",
    response = c("drat"),
    covariates = c("disp","mpg"),
    parameter = 3)


#caret package output
theirs<-train(drat~disp+mpg,data = mtcars,method = "knn",
              trControl=trainControl(method = "none"),
              tuneGrid=data.frame(k=3))

#Compare test MSE from these models
data.frame(datascience_testMSE=mine$test_MSE,
           caret_testMSE=sqrt(mean((mtcars$drat-predict(object = theirs,newdata = mtcars))^2)))
```

### CART (under development)

This is currently under development. Right now, the package can create a tree, but it seems to be splitting differently across the feature space than other common packages. 

Trees are known for being easier to understand than regression, which is steeped in theory. This is true, but regression is fantastically efficient. If you dig into the code for my package, you'll see that OLS can be accomplished in just a handful of steps. Regression trees, on the other hand, are difficult to vectorize since they are sequential. Furthermore, their high performance comes from a highly intensive search for optimal splits across the feature space. Trees test out each branch, covariance, and some subset of the range of each covariate. It's no wonder that individual trees overfit with ease. Such a search naturally translates to quite a bit of looping.

You can see that my code for this is just about a minimum viable process. There is clearly some opportunity for improvement: parallelizing branches, writing some Rcpp, etc. All I can say is hats off to the developers who have made CART implementations like from the ranger package so wickedly fast!

```{r}
#We'll start by making a decision tree
mine<-cart(data = mtcars,
    family = "tree",
    response = c("hp"),
    covariates = c("disp","mpg"),min_pre_partition_size = 10,min_terminal_node_size = 5,
    print_progress = F)

#Let's usee the "tree" package to compare
theirs<-tree(hp~ mpg + disp, 
             data = mtcars,control = tree.control(nrow(mtcars), mincut = 5, minsize = 10))


#It looks like the function I've supplied does not completely imitate the one from the three package just yet
cbind(mine[[1]]$hp,mine[[1]]$mean,predict(theirs))%>%data.frame%>%
  set_names(c("Actual Data","My Tree's Fit","Their Tree's Fit"))

```

